---
title: "Group Project"
author: "Group 12"
format: pdf
editor: visual
---

# Deliverables Group 12

# 1. Motivation

## 1.1 Research Motivation 

Since its launch in 2004, Yelp, as one of the largest online review platforms, plays a crucial role in shaping the reputations of businesses, particularly in the food industry. While there is an abundance of research analyzing Yelp reviews (*Agarwal, Pelullo, & Merchant 2019; Arthur, Etzioni, & Schwartz, 2019*), most studies focus on general patterns of consumer behavior (*Fogel, J. and Zachariah, S., 2017*), sentiment analysis (*Guerreiro & Rita, 2020*), or the detection of fake reviews (*Lee, Ham, Yang, & Koo, 2018*). What these studies largely overlook, however, is the significant role that influential users—those with elite status, numerous followers, or high review counts—play in shaping business ratings (*Pranata, I. and Susilo, W., 2016*). These users wield disproportionate influence on the platform, and their ratings are often perceived as more credible and trustworthy than those of regular users (*Tucker, T., 2011*). In competitive sectors like the food industry, where reputation can make or break a business, understanding the impact of these influential users is essential, as evidenced in study conducted by *Nakayama and Wan (2018)* about one-third of customers rely on online reviews when choosing a restaurant and over half of 18-to-34-year-olds factor reviews into their dining decisions.

Moreover, while the food industry on Yelp has been extensively studied (*Anderson and Magruder, 2012*) because of its tremendous impacts on business outcomes (*Luca, 2011*), the take-out restaurant niche has received minimal attention. The majority of existing studies on take-out restaurants emphasize negative aspects such as health risks, obesity, and food safety concerns (*Jeffery et al., 2006; Baek et al., 2022*). These studies tend to focus on the health implications of frequent take-out consumption, but fail to address the consumer dynamics on platforms like Yelp, where reviews have the power to influence public perception and business success. This gap in the literature calls for a deeper investigation into how influential users interact with take-out restaurants, especially as take-out has become increasingly important in the post-pandemic dining landscape.

This study aims to address this gap by asking: **Do influential Yelp users give higher ratings to take-out restaurants compared to non-influential users, and how do factors like location and business features affect these ratings?** This research is critical for several reasons. First, understanding the behavior of influential users could help businesses better manage their online reputations, particularly in a niche market like take-out dining. Second, the relationship between user influence, business attributes (such as parking availability or hours of operation), and location can provide key insights into consumer decision-making. For businesses operating in highly competitive environments, especially those without an established offline presence, this research could offer practical strategies for improving ratings and attracting new customers.

To explore these relationships, **regression analysis** will be employed as the primary research method. This method is well-suited for controlling multiple factors such as restaurant location, business attributes (e.g., parking, takeout options), and user metrics (e.g., yelping_since, fans, elite status). By using regression, we can isolate the effect of user influence on ratings and better understand the causal relationships at play. The analysis will focus on key variables like user profiles (including elite status, review count, and average stars), and business characteristics from the Yelp dataset. This approach will help to provide a clearer picture of how influential users impact the ratings of take-out restaurants compared to non-influential users.

Additionally, the accessibility and usefulness of the output from this study significantly benefit other students and the larger scientific community. By **developing an automated and reproducible workflow using open-source tools like R**, this research provides a template that others can easily adapt for similar analyses. **The study's findings and the associated code** can be shared on public platforms like GitHub, making them readily available for educational purposes and further research. The workflow includes data extraction, cleaning, transformation, and modeling processes, all documented and scripted to ensure transparency and repeatability. Moreover, the **comprehensive PDF report** serves as a valuable resource that clearly communicates the research methods, analyses, and findings. It can be used as a teaching tool in academic settings, demonstrating how to approach complex data analyses and interpret results within a real-world context.

## 1.2 Repisitory Structure and Documentation

## 1.3 Breadth of Contributions and Way-of-Working

# 2. Data Preparation & Analysis

## 2.1 Data Exploration

From the Yelp database the business, user, and review data files are downloaded to answer the research question.

------------------------------------------------------------------------

### 2.1.1 Extract a Yelp dataset in .tar format to separate JSON files

Install the archive package and load the necessary packages

```{r, echo=TRUE, eval=FALSE}
library(tidyverse)
```

Specify the path to your .tar file and verify if R can see the file

```{r, echo=TRUE, eval=FALSE}
tar_file <- "data/yelp_dataset.tar"
file.exists(tar_file)
```

List the contents of the .tar file

```{r, echo=TRUE, eval=FALSE}
contents <- archive::archive(tar_file)
print(contents)
```

Specify the extraction directory (make sure it is a directory)

```{r, echo=TRUE, eval=FALSE}
extract_dir <- "extracted_files"
```

Create the directory if it doesn't exist

```{r, echo=TRUE, eval=FALSE}
if (!dir.exists(extract_dir)) {
  dir.create(extract_dir)
}
```

Extract the files

```{r, echo=TRUE, eval=FALSE}
extracted_files <- archive::archive_extract(tar_file, dir = extract_dir)
print(extracted_files)
```

------------------------------------------------------------------------

### 2.1.2 Extract Yelp datasets in .JSON format to CSV files

This step involves using a different language, Python, to convert all the materials into the required format. The code below is written in Python:

**Coding: uft-8**

Convert the Yelp Dataset Challenge data set from json format to csv. For more information on the Yelp Dataset Challenge please visit <http://yelp.com/dataset_challenge>

```{python, echo=TRUE, eval=FALSE}
import argparse
import csv
import json
import os


def read_and_write_file(json_file, csv_file, column_names):
    with open(json_file, 'r', encoding='utf-8') as f, open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=column_names)
        writer.writeheader()
        for line in f:
            data = json.loads(line)
            writer.writerow(data)
    print(f"CSV file '{csv_file}' has been created successfully.")
    
def get_superset_of_column_names_from_file(json_file):
    column_names = set()
    with open(json_file, 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line)
            column_names.update(data.keys())
    return list(column_names)

def process_line(line_value):
    row = []
    if isinstance(line_value, str):
        row.append('{0}'.format(line_value))
    elif line_value is not None:
        row.append('{0}'.format(line_value))
    else:
        row.append('')
    return row


if __name__ == '__main__':
    # Convert a yelp dataset file from json to csv.
    print("Starting the conversion process...")

    parser = argparse.ArgumentParser(
        description='Convert Yelp Dataset Challenge data from JSON format to CSV.',
    )

    parser.add_argument(
        'json_file',
        type=str,
        help='The json file to convert.',
    )

    args = parser.parse_args()

    json_file = args.json_file
    csv_file = '{0}.csv'.format(json_file.split('.json')[0])

    # Print current working directory
    print(f"Current working directory: {os.getcwd()}")

    # Print absolute path of the JSON file
    abs_json_file_path = os.path.abspath(json_file)
    print(f"Absolute path of JSON file: {abs_json_file_path}")

    if not os.path.isfile(json_file):
        print(f"Error: The file '{json_file}' does not exist.")
        exit(1)

    print(f"Input JSON file: {json_file}")
    print(f"Output CSV file: {csv_file}")

    column_names = get_superset_of_column_names_from_file(json_file)
    print(f"Column names: {column_names}")

    read_and_write_file(json_file, csv_file, column_names)
    print("Conversion process completed.")
```

Finally, run the script from your terminal with the following command and replace yelp_academic_dataset.json with the path to the JSON file you wish to convert. The script will generate a CSV file with the same name as your input JSON file, located in the same directory

python json_to_csv_converter.py yelp_academic_dataset.json

------------------------------------------------------------------------

### 2.1.3 Inspect Yelp Data

**Inspecting the data**

The first step after loading in the data is to take a look at the different variables that are included in the three different data files.

------------------------------------------------------------------------

***Business_data***

The business dataset consists of the following variables:

| Variable     | Description                                                         | **Data class** |
|-------------|-----------------------------------------------|------------|
| business_id  | *A unique character string for each business*                       | Character      |
| name         | *The business name*                                                 | Character      |
| adress       | *The full address of the business*                                  | Character      |
| city         | *The city name where the business is located*                       | Character      |
| state        | *The state name where the business is located*                      | Character      |
| postal_code  | *The postal code of the business*                                   | Character      |
| latitude     | *The latitude of the business location*                             | Numeric        |
| longitude    | *The longitude of the business location*                            | Numeric        |
| stars        | *The average star rating rounded to half-stars*                     | Numeric        |
| review_count | *The number of reviews*                                             | Numeric        |
| is_open      | *Variable that shows with 0 or 1 if the business is closed or open* | Numeric        |
| attributes   | *Business attributes to values*                                     | Character      |
| categories   | *An array of different business categories*                         | Character      |
| hours        | *An object of key day to value hours, hours are using a 24hr clock* | Character      |

The different variables are evaluated and a selection of variables is chosen to keep in the data set to help answer the research question. These variables in the business data set include:

-   **business_id**

-   **city**

-   **state**

-   **stars**

-   **review_count**

-   **is_open**

-   **attributes**

------------------------------------------------------------------------

***User_data***

The user dataset consist of the following variables:

| Variable           | Description                                            | Data Class |
|---------------|-----------------------------------------------|----------|
| user_id            | *A unique character string as user identification*     | Character  |
| name               | *The user's first name*                                | Character  |
| review_count       | *The number of reviews written by an individual user*  | Numeric    |
| yelping_since      | *Date when user joined Yelp*                           | Character  |
| friends            | *An array of the user's friends as user id's*          | Character  |
| useful             | *Number of 'useful' votes send by user*                | Numeric    |
| funny              | *Number of 'funny' votes send by user*                 | Numeric    |
| cool               | *Number of 'cool' votes send by user*                  | Numeric    |
| fans               | *The number of fans a user has*                        | Numeric    |
| elite              | *The years the user had an 'elite' status on Yelp*     | Numeric    |
| average_stars      | *Average rating of all reviews*                        | Numeric    |
| compliment_hot     | *Number of 'hot' compliments received by the user*     | Numeric    |
| compliment_more    | *Number of 'more' compliments received by the user*    | Numeric    |
| compliment_profile | *Number of 'profile' compliments received by the user* | Numeric    |
| compliment_cute    | *Number of 'cute' compliments received by the user*    | Numeric    |
| compliment_list    | *Number of 'list' compliments received by the user*    | Numeric    |
| compliment_note    | *Number of 'note' compliments received by the user*    | Numeric    |
| compliment_plain   | *Number of 'plain' compliments received by the user*   | Numeric    |
| compliment_cool    | *Number of 'cool' compliments received by the user*    | Numeric    |
| compliment_funny   | *Number of 'funny' compliments received by the user*   | Numeric    |
| compliment_writer  | *Number of 'writer' compliments received by the user*  | Numeric    |
| compliment_photo   | *Number of 'photo' compliments received by the user*   | Numeric    |

The different variables are evaluated and a selection of variables is chosen to keep in the dataset to help answer the research question. These variables in the user dataset are:

-   **user_id**

-   **review_count**

-   **fans**

-   **elite**

------------------------------------------------------------------------

***review_data***

The review data consists of the following variables:

| Variable    | Description                                              | Data Class |
|---------------|-------------------------------------------|--------------|
| review_id   | *Unique review id*                                       | Character  |
| user_id     | *An unique character string as user id*                  | Character  |
| business_id | *A unique character string for each individual business* | Character  |
| stars       | *Star rating of the review*                              | Numeric    |
| date        | *Date of review, formatted YYYY-MM-DD*                   | Character  |
| text        | *The review text*                                        | Character  |
| useful      | *The number of 'useful' votes received*                  | Numeric    |
| funny       | *The number of 'funny' votes received*                   | Numeric    |
| Cool        | *The number of 'cool' votes received*                    | Numeric    |

The different variables are evaluated and a selection of variables is chosen to keep in the dataset to help answer the research question. These variables in the review dataset are:

-   **user_id**

-   **business_id**

-   **review_id**

-   **stars**

------------------------------------------------------------------------

## 2.2 Data Preparation

Because of the size of the Yelp data files and the time it takes to handle large data files, the decision has been made to prepare the data in two different phases.

[*Phase 1 data preparation consists of:*]{.underline}

1.  Downloading the Yelp data files from the internet.
2.  Loading these Yelp data files into the Rstudio environment.
3.  Inspecting data and selecting variables of interest.
4.  Merging the three data files into one merged file.
5.  Take a sample of the data called "sample_data" that will be used for cleaning and analysis.

[*Phase 2 data cleaning consists of:*]{.underline}

1.  Loading the sample data into a new R script
2.  Clean data
3.  Create new variables

------------------------------------------------------------------------

### 2.2.1 Preparing Data

**Loading libraries**

The first step of preparing the data is loading in the required R libraries.

```{r, echo=TRUE, eval=FALSE}
library(tidyverse) 
library(here)
```

------------------------------------------------------------------------

**Download data**

```{r, echo=TRUE, eval=FALSE}
### Needs to be added ###
```

------------------------------------------------------------------------

**Load data**

The first step is to load the different data files. To load these files, the libraries "tidyverse" and "here" were installed.

*Currently, the original Yelp data sets are loaded. This will later be changed to from the internet downloaded data sets*

```{r, echo=TRUE, eval=FALSE}
business_data <- read_csv(here("Data", "yelp_academic_dataset_business.csv"))
user_data <- read_csv(here("Data", "yelp_academic_dataset_user.csv"))
review_data <- read_csv(here("Data", "yelp_academic_dataset_review.csv"))
```

------------------------------------------------------------------------

**Filtering and Selecting Variables of interest in Data**

Because of the size of the three data files, with almost seven million observations in the review data, the files are filtered on usable variables before merging the data.

First, the business data file is filtered to remove unnecessary variables. This is done by using the `select()` function from the tidyverse library.

```{r, echo=TRUE, eval=FALSE}
business_data <- business_data %>% 
  select(business_id, name, city, state, stars, review_count, is_open,
         attributes)
```

Second, the user data file is filtered to remove unnecessary variables. This is done by using the `select()` function from the tidyverse library.

```{r, echo=TRUE, eval=FALSE}
user_data <- user_data %>% 
  select(user_id, name, review_count, yelping_since, fans, elite)
```

Third, the review data file is filtered to remove unnecessary variables. This is done by using the `select()` function from the tidyverse library.

```{r, echo=TRUE, eval=FALSE}
review_data <- review_data %>% 
  select(review_id, user_id, business_id, stars)

```

------------------------------------------------------------------------

**Merging the data files**

To combine the three different filtered data files, first, a `left_join()` was executed to combine the user data with the review data by the **"user_id"** variable.

```{r, echo=TRUE, eval=FALSE}
merge1 <- left_join(user_data, review_data, by = "user_id")
```

After combining these two data files, another `left_join()` was executed to include the business data to the final data file by the **"business_id"** variable.

```{r}
merged_data <- left_join(merge1, business_data, by = "business_id")
```

------------------------------------------------------------------------

**Saving the merged dataset**

Because we are working with large data files that take a long time to proces, it is important to save and create csv files while preparing the code. This is done below with the `write_csv()` function.

```{r}
write_csv(merged_data, here("data", "merged_data.csv"))
```

------------------------------------------------------------------------

**Taking a sample of the merged data**

Because of the large size of the merged data, with almost seven million observation, a sample of 50.000 observation is taken. This sample will be cleaned in the next phase.

To take make sure each sample that will be taken from the dataset is identical, the seed is set to 90 using `set.seed()`.

```{r, echo=TRUE, eval=FALSE}
set.seed(90)
```

For the next step, `slice_sample()` was used to take the sample of 50.000 observations.

```{r, echo=TRUE, eval=FALSE}
sample_data <- merged_data %>% 
  slice_sample(n = 50000)
```

------------------------------------------------------------------------

**Save the sample data**

To speed up the data handling process, a file called "sample_data.csv" is saved in the "data" file. This file will be opened in a new R script where the data will be cleaned for analysis.

```{r, echo=TRUE, eval=FALSE}
write_csv(sample_data, "sample_data.csv")
```

------------------------------------------------------------------------

### 2.2.2 Cleaning Data

**Loading libraries**

The first step of cleaning the data is loading in the required R libraries.

```{r, echo=TRUE, eval=FALSE}
library(tidyverse)
library(here)
```

------------------------------------------------------------------------

**Loading data**

The saved sample data is loaded from the "data" folder using the `here` library

```{r, echo=TRUE, eval=FALSE}
sample_data <- read_csv(here('data', 'sample_data.csv'))
```

------------------------------------------------------------------------

**Inspecting variable names in the dataset**

The `colnames()` function was used to check the different variable names.

```{r, echo=TRUE, eval=FALSE}
colnames(sample_data)
```

After running the code, it became clear that two of the variables were similar in different datasets. These were the "**stars**" and "**review_count**" variables. These variables were differentiated with a "**.x**" and "**.y**" at the end. In addition, the "**name**" variable seemed unclear about what its name indicated.

To separate the "**stars**" and "**review_count**", The variables were changed by using the `rename()` function to "**stars_user**", "**stars_business**", "**review_count_user**", and "**review_count_business**". To make clear what the "**name**" variable indicates, the variable name was changed to "**username**".

```{r, echo=TRUE, echo=FALSE}
sample_data <- sample_data %>% 
  rename(stars_user = stars.x,
         stars_business = stars.y,
         review_count_user = review_count.x,
         review_count_business = review_count.y,
         username = name)
```

Checking if the variable names are changed correctly

```{r, echo=TRUE, eval=FALSE}
colnames(sample_data)
```

------------------------------------------------------------------------

**Checking for NA values**

To check if there are any NA's in one of the variables the "**variable_na**" dataset is created as an overview of which variables include NA values.

```{r, echo=TRUE, eval=FALSE}
variable_na <- sample_data %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  pivot_longer(cols = everything(), 
               names_to = "variable", values_to = "na_count")

View(variable_na)
```

The "**variable_na**" table show that "**elite**" contains 37669 NA values and that "**attributes**" contains 1331 NA values.

The
